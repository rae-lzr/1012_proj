{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":67121,"databundleVersionId":7806901,"sourceType":"competition"},{"sourceId":7731345,"sourceType":"datasetVersion","datasetId":4517764},{"sourceId":7733314,"sourceType":"datasetVersion","datasetId":4518936},{"sourceId":11372,"sourceType":"modelInstanceVersion","modelInstanceId":5388}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center><img src=\"https://keras.io/img/logo-small.png\" alt=\"Keras logo\" width=\"100\"><br/>\nThis starter notebook is provided by the Keras team.</center>","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# LLM Prompt Recovery with [KerasNLP](https://github.com/keras-team/keras-nlp) and [Keras](https://github.com/keras-team/keras)\n\n<div align=\"center\">\n    <img src=\"https://i.ibb.co/8xZNc32/Gemma.png\">\n</div>\n\nIn this competition, the goal is to find the prompt used to transform a given text. Specifically, we're seeking the prompt or instruction used in the [Gemma 7B-it](https://www.kaggle.com/models/google/gemma/frameworks/pyTorch/variations/7b-it-quant) model to convert one text to another. Typically, large language models are instructed to transform one text to another style, but here we're tasked with the inverse: finding the instruction/prompt used for the transformation. This notebook walks you through fine-tuning the **Gemma 2b-it** model with LoRA for this prompt recovery task using KerasNLP. Witih KerasNLP, we can fine-tune with LoRA using just a few lines of code.\n\n**Fun fact**: This notebook is backend-agnostic, supporting TensorFlow, PyTorch, and JAX. However, the best performance can be achieved with `JAX`. KerasNLP and Keras enable the choice of preferred backend. Explore further details on [Keras](https://keras.io/keras_3/).\n\n**Note**: For a deeper understanding of KerasNLP, refer to the [KerasNLP guides](https://keras.io/keras_nlp/).","metadata":{}},{"cell_type":"markdown","source":"# Import Libraries ","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"KERAS_BACKEND\"] = \"jax\" # you can also use tensorflow or torch\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\" # avoid memory fragmentation on JAX backend.\n\nimport keras\nimport keras_nlp\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\ntqdm.pandas() # progress bar for pandas\n\nimport plotly.graph_objs as go\nimport plotly.express as px\nfrom IPython.display import display, Markdown","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-04-21T22:23:29.859698Z","iopub.execute_input":"2024-04-21T22:23:29.860063Z","iopub.status.idle":"2024-04-21T22:23:29.866376Z","shell.execute_reply.started":"2024-04-21T22:23:29.860035Z","shell.execute_reply":"2024-04-21T22:23:29.865482Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# Configuration","metadata":{}},{"cell_type":"code","source":"class CFG:\n    seed = 42\n    dataset_path = \"/kaggle/input/llm-prompt-recovery\"\n    preset = \"gemma_instruct_2b_en\" # name of pretrained Gemma\n    sequence_length = 512 # max size of input sequence for training\n    batch_size = 1 # size of the input batch in training\n    epochs = 1 # number of epochs to train","metadata":{"execution":{"iopub.status.busy":"2024-04-21T22:23:32.204699Z","iopub.execute_input":"2024-04-21T22:23:32.205269Z","iopub.status.idle":"2024-04-21T22:23:32.214199Z","shell.execute_reply.started":"2024-04-21T22:23:32.205052Z","shell.execute_reply":"2024-04-21T22:23:32.213147Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Reproducibility \nSets value for random seed to produce similar result in each run.","metadata":{}},{"cell_type":"code","source":"keras.utils.set_random_seed(CFG.seed)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T22:23:33.214355Z","iopub.execute_input":"2024-04-21T22:23:33.214713Z","iopub.status.idle":"2024-04-21T22:23:33.219303Z","shell.execute_reply.started":"2024-04-21T22:23:33.214685Z","shell.execute_reply":"2024-04-21T22:23:33.218264Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# Data\n\nNo training data is provided in this competition; in other words, we can use any openly available datasets for this competition. In this notebook, we will use two external datasets that utilize the **Gemma 7B** model to transform texts using prompts.\n\n**Data Format:**\n\nThese datasets includes:\n- `original_text`: Input text/essay that needs to be transformed.\n- `rewrite_prompt`: Prompt/Instruction that was used in the Gemma LM to transform `original_text`. This is also our **target** for this competition.\n- `rewritten_text`: Output text that was generated by the Gemma model.","metadata":{}},{"cell_type":"code","source":"# `LLM Prompt Recovery - Synthetic Datastore dataset` by @dschettler8845\ndf1 = pd.read_csv(\"/kaggle/input/llm-prompt-recovery-synthetic-datastore/gemma1000_w7b.csv\")\ndf1 = df1[[\"original_text\", \"rewrite_prompt\", \"gemma_7b_rewritten_text_temp0\"]]\ndf1 = df1.rename(columns={\"gemma_7b_rewritten_text_temp0\":\"rewritten_text\"})\ndf1.head(2)\n\n# `3000 Rewritten texts - Prompt recovery Challenge` by @dipamc77\ndf2 = pd.read_csv(\"/kaggle/input/3000-rewritten-texts-prompt-recovery-challenge/prompts_0_500_wiki_first_para_3000.csv\")\ndf2.head(2)\n\n# Merge all datasets\ndf = pd.concat([df1, df2], axis=0)\ndf = df.sample(2000).reset_index(drop=True) # to reduce training time we are only using 2k samples\ndf.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T22:23:39.297763Z","iopub.execute_input":"2024-04-21T22:23:39.298113Z","iopub.status.idle":"2024-04-21T22:23:39.420448Z","shell.execute_reply.started":"2024-04-21T22:23:39.298085Z","shell.execute_reply":"2024-04-21T22:23:39.419585Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"                                       original_text  \\\n0  Story highlights New findings suggest sperm al...   \n1  Glasgow's Needy is a non-registered charity or...   \n2  Pune: In a bizarre case, a parrot accused of \"...   \n3  Berrya is a genus of evergreen trees with fibr...   \n4  The Libertarian Party of Oregon is a political...   \n\n                                      rewrite_prompt  \\\n0                Turn this into a programmer's code.   \n1   Make this a formal apology letter to a customer.   \n2          Convert this into a technique to be used.   \n3  Convert the text into the layout of a classic ...   \n4  Recast it as the backstory for a mysterious an...   \n\n                                      rewritten_text  \n0  ```python\\n# Code to summarize the text\\n\\nspe...  \n1  [Your Name]\\n[Your Address]\\nGlasgow, Scotland...  \n2  **Technique:**\\n\\n**Step 1: Gather evidence.**...  \n3  ## Level Layout: Berrya Forest\\n\\n**Background...  \n4  In the shadows of the Evergreen State of Orego...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>original_text</th>\n      <th>rewrite_prompt</th>\n      <th>rewritten_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Story highlights New findings suggest sperm al...</td>\n      <td>Turn this into a programmer's code.</td>\n      <td>```python\\n# Code to summarize the text\\n\\nspe...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Glasgow's Needy is a non-registered charity or...</td>\n      <td>Make this a formal apology letter to a customer.</td>\n      <td>[Your Name]\\n[Your Address]\\nGlasgow, Scotland...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Pune: In a bizarre case, a parrot accused of \"...</td>\n      <td>Convert this into a technique to be used.</td>\n      <td>**Technique:**\\n\\n**Step 1: Gather evidence.**...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Berrya is a genus of evergreen trees with fibr...</td>\n      <td>Convert the text into the layout of a classic ...</td>\n      <td>## Level Layout: Berrya Forest\\n\\n**Background...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The Libertarian Party of Oregon is a political...</td>\n      <td>Recast it as the backstory for a mysterious an...</td>\n      <td>In the shadows of the Evergreen State of Orego...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Prompt Engineering\n\nHere's a simple prompt template we'll use to create instruction-response pairs from the `original_text`, `rewritten_text`, and `rewritten_prompt`:\n\n```\nInstruction:\nBelow, the `Original Text` passage has been rewritten/transformed/improved into `Rewritten Text` by the `Gemma 7b-it` LLM with a certain prompt/instruction. Your task is to carefully analyze the differences between the \"Original Text\" and \"Rewritten Text\", and try to infer the specific prompt or instruction that was likely given to the LLM to rewrite/transform/improve the text in this way.\n\nOriginal Text: \n...\n\nRewritten Text:\n...\n\nResponse:\n...\n```\n\nThis template will help the model to follow instruction and respond accurately. You can explore more advanced prompt templates for better results.","metadata":{}},{"cell_type":"code","source":"template = \"\"\"Instruction:\\nBelow, the `Original Text` passage has been rewritten/transformed/improved into `Rewritten Text` by the `Gemma 7b-it` LLM with a certain prompt/instruction. Your task is to carefully analyze the differences between the `Original Text` and `Rewritten Text`, and try to infer the specific prompt or instruction that was likely given to the LLM to rewrite/transform/improve the text in this way.\\n\\nOriginal Text:\\n{original_text}\\n\\nRewriten Text:\\n{rewritten_text}\\n\\nResponse:\\n{rewrite_prompt}\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-04-21T22:23:39.821941Z","iopub.execute_input":"2024-04-21T22:23:39.822816Z","iopub.status.idle":"2024-04-21T22:23:39.827065Z","shell.execute_reply.started":"2024-04-21T22:23:39.822777Z","shell.execute_reply":"2024-04-21T22:23:39.826061Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"template2 = \"\"\"Instruction:\\nBelow, the `Original Text` passage has been summarized/paraphrased/expanded/simplified into `Rewritten Text` by the `Gemma 7b-it` LLM with a certain prompt/instruction. Your task is to carefully analyze the differences between the `Original Text` and `Rewritten Text`, and try to infer the specific prompt or instruction that was likely given to the LLM to summarize/paraphrase/expand/simplify the text in this way.\\n\\nOriginal Text:\\n{original_text}\\n\\nRewriten Text:\\n{rewritten_text}\\n\\nResponse:\\n{rewrite_prompt}\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-04-21T22:23:40.044404Z","iopub.execute_input":"2024-04-21T22:23:40.045105Z","iopub.status.idle":"2024-04-21T22:23:40.049592Z","shell.execute_reply.started":"2024-04-21T22:23:40.045074Z","shell.execute_reply":"2024-04-21T22:23:40.048437Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"df[\"prompt\"] = df.progress_apply(lambda row: template.format(original_text=row.original_text,\n                                                             rewritten_text=row.rewritten_text,\n                                                             rewrite_prompt=row.rewrite_prompt), axis=1)\ndata = df.prompt.tolist()","metadata":{"execution":{"iopub.status.busy":"2024-04-21T22:23:40.220420Z","iopub.execute_input":"2024-04-21T22:23:40.220772Z","iopub.status.idle":"2024-04-21T22:23:40.324764Z","shell.execute_reply.started":"2024-04-21T22:23:40.220744Z","shell.execute_reply":"2024-04-21T22:23:40.323800Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a35410c6b704f91b711a5620aa0c77e"}},"metadata":{}}]},{"cell_type":"markdown","source":"Let's examine a sample prompt. As the answers in our dataset are curated with **markdown** format, we will render the sample using `Markdown()` to properly visualize the formatting.","metadata":{}},{"cell_type":"markdown","source":"## Sample","metadata":{}},{"cell_type":"code","source":"def colorize_text(text):\n    for word, color in zip([\"Instruction\", \"Original Text\", \"Rewriten Text\", \"Response\"],\n                           [\"red\", \"yellow\", \"blue\", \"green\"]):\n        text = text.replace(f\"{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n    return text","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-04-21T22:23:42.850138Z","iopub.execute_input":"2024-04-21T22:23:42.851066Z","iopub.status.idle":"2024-04-21T22:23:42.855790Z","shell.execute_reply.started":"2024-04-21T22:23:42.851029Z","shell.execute_reply":"2024-04-21T22:23:42.854869Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Take a random sample\nsample = data[10]\n\n# Give colors to Instruction, Response and Category\nsample = colorize_text(sample)\n\n# Show sample in markdown\ndisplay(Markdown(sample))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-04-21T22:23:43.247814Z","iopub.execute_input":"2024-04-21T22:23:43.248178Z","iopub.status.idle":"2024-04-21T22:23:43.256059Z","shell.execute_reply.started":"2024-04-21T22:23:43.248150Z","shell.execute_reply":"2024-04-21T22:23:43.255177Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"\n\n**<font color='red'>Instruction:</font>**\nBelow, the `Original Text` passage has been rewritten/transformed/improved into `Rewritten Text` by the `Gemma 7b-it` LLM with a certain prompt/instruction. Your task is to carefully analyze the differences between the `Original Text` and `Rewritten Text`, and try to infer the specific prompt or instruction that was likely given to the LLM to rewrite/transform/improve the text in this way.\n\n\n\n**<font color='yellow'>Original Text:</font>**\nZolt치n T칩th (born 24 August 1979) is a Hungarian former competitive figure skater. He is a five-time Hungarian national champion and competed in two Winter Olympics.\n\n\n\n**<font color='blue'>Rewriten Text:</font>**\nSure, here's the rewritten text as if it were a list of ingredients in a magical potion:\n\nThe ingredients to create the potion:\n\nZolt치n T칩th's birth date: 24 August 1979\nHungarian heritage\nFive-time Hungarian national champion\nTwo Winter Olympics participation\n\n\n\n**<font color='green'>Response:</font>**\nRewrite the text as if it were a list of ingredients in a magical potion."},"metadata":{}}]},{"cell_type":"markdown","source":"# Modeling\n\n<div align=\"center\"><img src=\"https://i.ibb.co/Bqg9w3g/Gemma-Logo-no-background.png\" width=\"300\"></div>\n\n**Gemma** is a collection of advanced open LLMs developed by **Google DeepMind** and other **Google teams**, derived from the same research and technology behind the **Gemini** models. They can be integrated into applications and run on various platforms including mobile devices and hosted services. Developers can customize Gemma models using tuning techniques to enhance their performance for specific tasks, offering more targeted and efficient generative AI solutions beyond text generation.\n\nGemma models are available in several sizes so we can build generative AI solutions based on your available computing resources, the capabilities you need, and where you want to run them.\n\n| Parameters size | Tuned versions    | Intended platforms                 | Preset                 |\n|-----------------|-------------------|------------------------------------|------------------------|\n| 2B              | Pretrained        | Mobile devices and laptops         | `gemma_2b_en`          |\n| 2B              | Instruction tuned | Mobile devices and laptops         | `gemma_instruct_2b_en` |\n| 7B              | Pretrained        | Desktop computers and small servers| `gemma_7b_en`          |\n| 7B              | Instruction tuned | Desktop computers and small servers| `gemma_instruct_7b_en` |\n\nIn this notebook, we will utilize the `Gemma 2b-it` model from KerasNLP's pretrained models to recover the prompt. We are using the \"Instruction tuned\" model instead of the \"Pretrained\" one because the test data was generated from an instruction-tuned Gemma model. Additionally, we will fine-tune our model using instruction-response pairs thus fine-tuning an instruction-tuned model will likely yield better results.\n\nTo explore other available models, you can simply adjust the `preset` value in the `CFG` (config). You can find a list of other pretrained models on the [KerasNLP website](https://keras.io/api/keras_nlp/models/).","metadata":{}},{"cell_type":"markdown","source":"## Gemma Causal LM\n\nThe code below will build an end-to-end Gemma model for causal language modeling (hence the name `GemmaCausalLM`). A causal language model (LM) predicts the next token based on previous tokens. This task setup can be used to train the model unsupervised on plain text input or to autoregressively generate plain text similar to the data used for training. This task can be used for pre-training or fine-tuning a Gemma model simply by calling `fit()`.\n\nThis model has a `generate()` method, which generates text based on a prompt. The generation strategy used is controlled by an additional sampler argument on `compile()`. You can recompile the model with different `keras_nlp.samplers` objects to control the generation. By default, `\"greedy\"` sampling will be used.\n\n> The `from_preset` method instantiates the model from a preset architecture and weights.","metadata":{}},{"cell_type":"code","source":"gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(CFG.preset)\ngemma_lm.summary()","metadata":{"execution":{"iopub.status.busy":"2024-04-21T22:23:44.601591Z","iopub.execute_input":"2024-04-21T22:23:44.601945Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Attaching 'config.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nMounting files to /kaggle/input/gemma/keras/gemma_instruct_2b_en/2...\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Gemma LM Preprocessor\n\nAn important part of the Gemma model is the **Preprocessor** layer, which under the hood uses **Tokenizer**.\n\n**What it does:** The preprocessor takes input strings and transforms them into a dictionary (`token_ids`, `padding_mask`) containing preprocessed tensors. This process starts with tokenization, where input strings are converted into sequences of token IDs.\n\n**Why it's important:** Initially, raw text data is complex and challenging for modeling due to its high dimensionality. By converting text into a compact set of tokens, such as transforming `\"The quick brown fox\"` into `[\"the\", \"qu\", \"##ick\", \"br\", \"##own\", \"fox\"]`, we simplify the data. Many models rely on special tokens and additional tensors to understand input. These tokens help divide input and identify padding, among other tasks. Making all sequences the same length through padding boosts computational efficiency, making subsequent steps smoother.\n\nExplore the following pages to access the available preprocessing and tokenizer layers in **KerasNLP**:\n- [Preprocessing](https://keras.io/api/keras_nlp/preprocessing_layers/)\n- [Tokenizers](https://keras.io/api/keras_nlp/tokenizers/)","metadata":{}},{"cell_type":"code","source":"x, y, sample_weight = gemma_lm.preprocessor(data[0:2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This preprocessing layer will take in batches of strings, and return outputs in a `(x, y, sample_weight)` format, where the `y` label is the next token id in the `x` sequence.\n\nFrom the code below, we can see that, after the preprocessor, the data shape is `(num_samples, sequence_length)`.","metadata":{}},{"cell_type":"code","source":"# Display the shape of each processed output\nfor k, v in x.items():\n    print(k, \":\", v.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference before Fine-Tuning\n\nBefore we do fine-tuning, let's try to recover the prompt using the Gemma model with some prepared prompts and see how it responds.\n\n> As this model is not yet fine-tuned for instruction, you will notice that the model's responses are inaccurate.","metadata":{}},{"cell_type":"markdown","source":"## Sample 1","metadata":{}},{"cell_type":"code","source":"# Take one sample\nrow = df.iloc[10]\n\n# Generate Prompt using template\nprompt = template.format(\n    original_text=row.original_text,\n    rewritten_text=row.rewritten_text,\n    rewrite_prompt=\"\",\n)\n\n# Infer\noutput = gemma_lm.generate(prompt, max_length=512)\n\n# Colorize\noutput = colorize_text(output)\n\n# Display in markdown\ndisplay(Markdown(output))\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sample 2","metadata":{}},{"cell_type":"code","source":"# Take one sample\nrow = df.iloc[20]\n\n# Generate Prompt using template\nprompt = template.format(\n    original_text=row.original_text,\n    rewritten_text=row.rewritten_text,\n    rewrite_prompt=\"\",\n)\n\n# Infer\noutput = gemma_lm.generate(prompt, max_length=512)\n\n# Colorize\noutput = colorize_text(output)\n\n# Display in markdown\ndisplay(Markdown(output))\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fine-tuning with LoRA\n\nTo get better responses from the model, we will fine-tune the model with Low Rank Adaptation (LoRA).\n\n**What exactly is LoRA?**\n\nLoRA is a method used to fine-tune large language models (LLMs) in an efficient way. It involves freezing the weights of the LLM and injecting trainable rank-decomposition matrices.\n\nImagine in an LLM, we have a pre-trained dense layer, represented by a $d \\times d$ weight matrix, denoted as $W_0$. We then initialize two additional dense layers, labeled as $A$ and $B$, with shapes $d \\times r$ and $r \\times d$, respectively. Here, $r$ denotes the rank, which is typically **much smaller than** $d$. Prior to LoRA, the model's output was computed using the equation $output = W_0 \\cdot x + b_0$, where $x$ represents the input and $b_0$ denotes the bias term associated with the original dense layer, which remains frozen. After applying LoRA, the equation becomes $output = (W_0 \\cdot x + b_0) + (B \\cdot A \\cdot x)$, where $A$ and $B$ denote the trainable rank-decomposition matrices that have been introduced.\n\n<center><img src=\"https://i.ibb.co/DWsbhLg/LoRA.png\" width=\"300\"><br/>\nCredit: <a href=\"https://arxiv.org/abs/2106.09685\">LoRA: Low-Rank Adaptation of Large Language Models</a> Paper</center>\n\n\nIn the LoRA paper, $A$ is initialized with $\\mathcal{N} (0, \\sigma^2)$ and $B$ with $0$, where $\\mathcal{N}$ denotes the normal distribution, and $\\sigma^2$ is the variance.\n\n**Why does LoRA save memory?**\n\nEven though we're adding more layers to the model with LoRA, it actually helps save memory. This is because the smaller layers (A and B) have fewer parameters to learn compared to the big model and fewer trainable parameters mean fewer optimizer variables to store. So, even though the overall model might seem bigger, it's actually more efficient in terms of memory usage. \n\n> This notebook uses a LoRA rank of `4`. A higher rank means more detailed changes are possible, but also means more trainable parameters.","metadata":{}},{"cell_type":"code","source":"# Enable LoRA for the model and set the LoRA rank to 4.\ngemma_lm.backbone.enable_lora(rank=4)\ngemma_lm.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Notice** that, the number of trainable parameters is reduced from ~$2.5$ billions to ~$1.3$ millions after enabling LoRA.","metadata":{}},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"# Limit the input sequence length to 512 (to control memory usage).\ngemma_lm.preprocessor.sequence_length = CFG.sequence_length \n\n# Compile the model with loss, optimizer, and metric\ngemma_lm.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=keras.optimizers.Adam(learning_rate=3e-5),\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\n\n# Train model\ngemma_lm.fit(data, epochs=CFG.epochs, batch_size=CFG.batch_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference after fine-tuning\n\nLet's see how our fine-tuned model responds to the same questions we asked before fine-tuning the model.","metadata":{}},{"cell_type":"markdown","source":"## Sample 1","metadata":{}},{"cell_type":"code","source":"# Take one sample\nrow = df.iloc[10]\n\n# Generate Prompt using template\nprompt = template.format(\n    original_text=row.original_text,\n    rewritten_text=row.rewritten_text,\n    rewrite_prompt=\"\",\n)\n\n# Infer\noutput = gemma_lm.generate(prompt, max_length=512)\n\n# Colorize\noutput = colorize_text(output)\n\n# Display in markdown\ndisplay(Markdown(output))","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sample 2","metadata":{}},{"cell_type":"code","source":"# Take one sample\nrow = df.iloc[20]\n\n# Generate Prompt using template\nprompt = template.format(\n    original_text=row.original_text,\n    rewritten_text=row.rewritten_text,\n    rewrite_prompt=\"\",\n)\n\n# Infer\noutput = gemma_lm.generate(prompt, max_length=512)\n\n# Colorize\noutput = colorize_text(output)\n\n# Display in markdown\ndisplay(Markdown(output))\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test Data","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv(\"/kaggle/input/llm-prompt-recovery/test.csv\")\ntest_df['original_text'] = test_df['original_text'].fillna(\"\")\ntest_df['rewritten_text'] = test_df['rewritten_text'].fillna(\"\")\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-08T19:26:54.434613Z","iopub.execute_input":"2024-03-08T19:26:54.43503Z","iopub.status.idle":"2024-03-08T19:26:54.448795Z","shell.execute_reply.started":"2024-03-08T19:26:54.434999Z","shell.execute_reply":"2024-03-08T19:26:54.447604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test Sample\n\nNow, let's try out a sample from test data that model hasn't seen during training.","metadata":{}},{"cell_type":"code","source":"row = test_df.iloc[0]\n\n# Generate Prompt using template\nprompt = template.format(\n    original_text=row.original_text,\n    rewritten_text=row.rewritten_text,\n    rewrite_prompt=\"\",\n)\n\n# Infer\noutput = gemma_lm.generate(prompt, max_length=512)\n\n# Colorize\noutput = colorize_text(output)\n\n# Display in markdown\ndisplay(Markdown(output))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-08T19:27:29.022534Z","iopub.execute_input":"2024-03-08T19:27:29.023234Z","iopub.status.idle":"2024-03-08T19:27:30.390404Z","shell.execute_reply.started":"2024-03-08T19:27:29.023199Z","shell.execute_reply":"2024-03-08T19:27:30.389393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"preds = []\nfor i in tqdm(range(len(test_df))):\n    row = test_df.iloc[i]\n\n    # Generate Prompt using template\n    prompt = template.format(\n        original_text=row.original_text,\n        rewritten_text=row.rewritten_text,\n        rewrite_prompt=\"\"\n    )\n\n    # Infer\n    output = gemma_lm.generate(prompt, max_length=512)\n    pred = output.replace(prompt, \"\") # remove the prompt from output\n    \n    # Store predictions\n    preds.append([row.id, pred])","metadata":{"execution":{"iopub.status.busy":"2024-03-08T19:28:43.847896Z","iopub.execute_input":"2024-03-08T19:28:43.848333Z","iopub.status.idle":"2024-03-08T19:28:45.843319Z","shell.execute_reply.started":"2024-03-08T19:28:43.848301Z","shell.execute_reply":"2024-03-08T19:28:45.842148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"While preparing the submission file, we must keep in mind that, leaving any `rewrite_prompt` blank as null answers will throw an error.","metadata":{}},{"cell_type":"code","source":"sub_df = pd.DataFrame(preds, columns=[\"id\", \"rewrite_prompt\"])\nsub_df['rewrite_prompt'] = sub_df['rewrite_prompt'].fillna(\"\")\nsub_df['rewrite_prompt'] = sub_df['rewrite_prompt'].map(lambda x: \"Improve the essay\" if len(x) == 0 else x)\nsub_df.to_csv(\"submission.csv\",index=False)\nsub_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-08T19:29:44.702671Z","iopub.execute_input":"2024-03-08T19:29:44.703073Z","iopub.status.idle":"2024-03-08T19:29:44.721731Z","shell.execute_reply.started":"2024-03-08T19:29:44.703039Z","shell.execute_reply":"2024-03-08T19:29:44.720264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n\nThe result is pretty good. Still there is ample room for improvement. Here are some tips to improve performance:\n\n- Try using the larger version of **Gemma** (7B).\n- Increase `sequence_length`.\n- Experiment with advanced prompt engineering techniques.\n- Implement augmentation to increase the number of samples.\n- Utilize a learning rate scheduler.","metadata":{}},{"cell_type":"markdown","source":"# Reference\n* [Fine-tune Gemma models in Keras using LoRA](https://www.kaggle.com/code/nilaychauhan/fine-tune-gemma-models-in-keras-using-lora)\n* [Parameter-efficient fine-tuning of GPT-2 with LoRA](https://keras.io/examples/nlp/parameter_efficient_finetuning_of_gpt2_with_lora/)\n* [Gemma - KerasNLP](https://keras.io/api/keras_nlp/models/gemma/)","metadata":{}}]}